# MyChatAI

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.9%2B-blue?style=flat-square" alt="python">
  <img src="https://img.shields.io/badge/License-MIT-green?style=flat-square" alt="license">
  <img src="https://img.shields.io/badge/CI-GitHub_Actions-blue?style=flat-square" alt="ci">
</div>

**A lightweight, productionâ€‘ready orchestration layer that lets you swap between cloud LLMs (OpenAI, Anthropic, etc.) and local Ollama models with a single line of code.**

---

## TableÂ ofÂ Contents

1. [Features](#features)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Quickâ€‘Start](#quick-start)
5. [Configuration](#configuration)
6. [Usage](#usage)

   * CLI
   * Python SDK
7. [Testing](#testing)
8. [Troubleshooting](#troubleshooting)
9. [Roadmap](#roadmap)
10. [Contributing](#contributing)
11. [License](#license)

---

## âœ¨ Features<a id="features"></a>

| Category                   | What you get                                                                                                             |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Unified ClientÂ API**     | Inject a different `AbstractModelClient` to switch from OpenAI â†’ Ollama (or any future backend) â€” no other code changes. |
| **Streaming Support**      | Tokenâ€‘level streaming outâ€‘ofâ€‘theâ€‘box for both providers.                                                                 |
| **TypedÂ Settings**         | Centralised config via *pydanticâ€‘settings*; `.env` loading included.                                                     |
| **Pluggable**              | Strategy pattern makes it trivial to add *AnthropicClient*, *GroqClient*, etc.                                           |
| **100Â % EditableÂ Install** | `pip install -e .` for instant hotâ€‘reload during development.                                                            |
| **CIâ€‘Ready**               | Linting (`ruff`), typing (`mypy`), tests (`pytest`), and GitHubÂ Actions template.                                        |
| **Zero GlobalÂ State**      | No more `openai.api_key = ...` mutations; each client owns its own connection.                                           |

---

## ğŸ— Architecture<a id="architecture"></a>

```text
mychatai/
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ base.py          # AbstractModelClient (interface)
â”‚   â”œâ”€â”€ openai.py        # OpenAIClient
â”‚   â””â”€â”€ ollama.py        # OllamaClient
â”œâ”€â”€ prompts.py           # Reads system_prompt.txt, builds user prompts
â”œâ”€â”€ chat_service.py      # Highâ€‘level faÃ§ade / orchestration
â”œâ”€â”€ config.py            # Pydanticâ€‘settings singleton
â””â”€â”€ utils/               # Display helpers (e.g. streaming in notebooks)
```

```ascii
                +------------------+
 User â†’ CLI â†’   | ChatService      |
                +--------+---------+
                         | uses
                         â–¼
                +------------------+      +------------------+
                | OpenAIClient     |â—€â”€â”€â”  | OllamaClient     |
                +------------------+   â”‚  +------------------+
                         â–²             â”‚         â–²
                         â”‚             â”‚ uses    â”‚ uses
                +--------+---------+   â”‚  +------+-------+
                | AbstractModelClient |â”€â”˜  | HTTP / SDK |
                +--------------------+      +-------------+
```

---

## ğŸ›  Installation<a id="installation"></a>

```bash
# 1ï¸âƒ£  Clone
 git clone https://github.com/yourâ€‘org/mychatai.git && cd mychatai

# 2ï¸âƒ£  Create & activate virtualâ€‘env
 python -m venv .venv && source .venv/bin/activate

# 3ï¸âƒ£  Install
 pip install -e .[dev]
```

---

## ğŸš€ Quickâ€‘Start<a id="quick-start"></a>

### Provide credentials (choose **one** provider)

```bash
# OpenAI         (cloud)
export OPENAI_API_KEY="skâ€‘..."

# Ollama (local) (make sure `ollama serve` is running)
export OLLAMA_URL="http://localhost:11434/api/chat"
export MYCHATAI_OLLAMA_MODEL="phi3"
```

### Ask a question from the CLI

```bash
./scripts/ask.py "What is camera calibration?"

# or via entryâ€‘point
ask -p ollama --no-stream "What is camera calibration?"
```

### Use it in Python

```python
from mychatai import ChatService, OpenAIClient
chat = ChatService(OpenAIClient())
print(chat.answer("Give me a dad joke about resistors.", stream=False))
```

Streaming:

```python
from mychatai import ChatService, OllamaClient
from mychatai.utils.display import stream_to_stdout

chat   = ChatService(OllamaClient())
tokens = chat.answer("Explain forwardâ€‘propagation in 100 words.", stream=True)
stream_to_stdout(tokens)
```

---

## âš™ï¸ Configuration<a id="configuration"></a>

All runtime options are exposed via **envâ€‘vars** and stronglyâ€‘typed in `config.py`.

| Variable                   | Default                           | Description                        |
| -------------------------- | --------------------------------- | ---------------------------------- |
| `OPENAI_API_KEY`           | â€”                                 | API key for OpenAI.                |
| `OLLAMA_URL`               | `http://localhost:11434/api/chat` | Ollama REST endpoint.              |
| `MYCHATAI_OLLAMA_MODEL`    | `llma3.2`                         | Default local model.               |
| `MYCHATAI_REQUEST_TIMEOUT` | `60` (seconds)                    | Network timeout for all providers. |

Add them to a local `.env` file for convenience; *pydanticâ€‘settings* will autoâ€‘load it.

---

## ğŸ§ª Testing<a id="testing"></a>

```bash
pytest -q          # unit tests (mocks)
pytest -m live     # live tests (needs real API keys)
ruff check .       # lint
mypy mychatai      # type checks
```

---

## ğŸ©¹ Troubleshooting<a id="troubleshooting"></a>

| Symptom                              | Fix                                                                            |
| ------------------------------------ | ------------------------------------------------------------------------------ |
| `ModuleNotFoundError: mychatai`      | Activate the venv and run `pip install -e .` again.                            |
| Shebang errors (`command not found`) | Ensure `scripts/ask.py` starts with `#!/usr/bin/env python` and `chmod +x` it. |
| Pylance red squiggles                | Use **PythonÂ â–¶ SelectÂ Interpreter** and choose the venv.                       |
| Long delay / timeout                 | Increase `request_timeout` in `config.py` or check proxy settings.             |
| â€œmodel not foundâ€ (Ollama)           | `ollama pull <model>` â€” e.g., `ollama pull phi3`.                              |

---

## ğŸ—º Roadmap<a id="roadmap"></a>

* ğŸ”Œ **AnthropicClient** â€“ add Claude support.
* â™»ï¸ **Async API** â€“ optional async/await variant of `ChatService`.
* ğŸ“ **Docs Site** â€“ GitHubÂ Pages with rich examples.
* ğŸ³ **Docker image** â€“ `docker run mychatai` for quick demos.

---

## ğŸ¤ Contributing<a id="contributing"></a>

1. ForkÂ â†’ branch â†’ PR.
2. Run `preâ€‘commit run --all-files` before pushing.
3. Write tests for new functionality.
4. Ensure CI is green.

We follow the [ConventionalÂ Commits](https://www.conventionalcommits.org/) spec and *semantic versioning*.

---

## ğŸ“„ License<a id="license"></a>

`mychatai` is released under the **MIT License**.  See [`LICENSE`](LICENSE) for details.
